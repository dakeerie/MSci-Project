{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c87921",
   "metadata": {},
   "source": [
    "# Torch\n",
    "### Notes and code based on youtube videos by user Patrick Loeber: https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73cac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21efc012",
   "metadata": {},
   "source": [
    "### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ff3756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([[ 0,  0],\n",
      "        [10, 17]]) y =  tensor([[1, 4],\n",
      "        [4, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(20, size=(2,2))\n",
    "y = torch.randint(6, size=(2,2))\n",
    "\n",
    "print('x = ', x, 'y = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36f705",
   "metadata": {},
   "source": [
    "Element-wise operations written as $\\textit{x+y, x-y, x*y, x/y}$ or $\\textit{x.add(y)}$. Add a trailing underscore to do the operation in place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7a48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  4],\n",
      "        [14, 17]])\n",
      "tensor([[ 1,  4],\n",
      "        [14, 17]])\n",
      "Same thing.\n"
     ]
    }
   ],
   "source": [
    "print(x + y)\n",
    "y.add_(x)\n",
    "print(y)\n",
    "if y.all() == (x+y).all():\n",
    "    print(\"Same thing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32066550",
   "metadata": {},
   "source": [
    "Can use .item() to extract (scalar) tensor component as long as the tensor is sliced properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de1d260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 1],\n",
      "        [0, 4]])\n",
      "tensor(1) 1\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "z = torch.randint(10, (2, 2))\n",
    "print(z)\n",
    "scalar = z[0, 1]\n",
    "print(scalar, scalar.item())\n",
    "print(type(scalar), type(scalar.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157008a",
   "metadata": {},
   "source": [
    "$\\textit{t.view()}$ can be used to reshape a tensor. Reshape dimensions must have the same product as previous dimension product ie $m \\times n$ maps to $a \\times b$ where $mn = ab$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fb6721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2115, 0.9093, 0.9184, 0.9050],\n",
      "        [0.2088, 0.7229, 0.1240, 0.6312],\n",
      "        [0.7044, 0.6910, 0.4384, 0.0906],\n",
      "        [0.1265, 0.3170, 0.5314, 0.5708]])\n",
      "tensor([[0.2115, 0.9093],\n",
      "        [0.9184, 0.9050],\n",
      "        [0.2088, 0.7229],\n",
      "        [0.1240, 0.6312],\n",
      "        [0.7044, 0.6910],\n",
      "        [0.4384, 0.0906],\n",
      "        [0.1265, 0.3170],\n",
      "        [0.5314, 0.5708]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4, 4)\n",
    "print(a)\n",
    "b = a.view(-1, 2) #reshape -1 makes automatic choice of other dimension when given one dimension as\n",
    "#next input\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f4a6b",
   "metadata": {},
   "source": [
    "Can turn a numpy array into a torch tensor and vice versa. Care needs to be taken though as the tensor and array both occupy the same memory location if only working with the CPU (not GPU) so changes to one are also changes to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "840eb305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'numpy.ndarray'>\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "o = torch.ones(5)\n",
    "print(o)\n",
    "p = o.numpy() \n",
    "print(type(p)) \n",
    "\n",
    "s = np.ones(5)\n",
    "print(s)\n",
    "t = torch.from_numpy(s)\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae3fef",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0df5e",
   "metadata": {},
   "source": [
    "When initialising a tensor, setting the $\\textit{requires\\_grad}$ equal to True let's torch know that you will want to take the gradient of this tensor later on. This attribute needs to be specified before back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65957e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.], requires_grad=True)\n",
      "tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0.0,1.0,2.0], requires_grad=True) \n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1f92c",
   "metadata": {},
   "source": [
    "Since requires_grad is True, torch tracks the operations made in the grad_fn attribute of the tensors and creates a computation graph for back propagation later. The forward pass calculates the output $y = f(x)$ and since gradient is specified, torch automatically creates a function for us which is used in back propagation to calculate the gradient. $y$ has attribute grad_fn which points to gradient function dy/dx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83bca362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.], requires_grad=True)\n",
      "tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n",
      "tensor([ 8., 18., 32.], grad_fn=<MulBackward0>)\n",
      "tensor([24.8000, 48.0000, 96.0160])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "z = y*y*2\n",
    "print(z)\n",
    "v = torch.tensor([1.0, 1.0, 2.0], dtype = torch.float32)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb08c7",
   "metadata": {},
   "source": [
    "$\\textit{z.backward()}$ calculates the derivative of z wrt the first tensor in the branch chain ie calculates dz/dx. However, $\\textit{z.backward()}$, doesn't work by itself unless the output is a scalar- it throws the error \"grad can be implicitly created only for scalar outputs\". For it to work you need to perform the vector jacobian product or similar higher dimensional product with the correct rank tensor to output the gradient. The vector jacobian product is similar to a change of basis matrix transformation. This is done by writing $\\textit{z.backward(v)}$ where $v$ is the correct rank tensor required for a properly defined product. Not sure about how the components of v are picked from a neural networks perspective but in the usual jacobian product it usually contains the primary basis coordinates ~.\n",
    "\n",
    "The $\\textit{.grad}$ part outputs a tensor of same rank as the input ($x$ here) containing the \"vector\" in the vector jacobian product, usually the pre-computated gradients wrt each output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729928fc",
   "metadata": {},
   "source": [
    "Actions can be taken to allow a tensor to be created without being added to the branch diagram. This prevents torch from tracking history and calculating the grad_fn attribute allowing for adjustment of the neural network. Can be done in three ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac381d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6850, -0.8555, -0.5232], requires_grad=True)\n",
      "tensor([ 0.6850, -0.8555, -0.5232])\n",
      "tensor([ 0.6850, -0.8555, -0.5232], requires_grad=True)\n",
      "tensor([ 0.6850, -0.8555, -0.5232])\n",
      "tensor([2.6850, 1.1445, 1.4768])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(3, requires_grad=True)\n",
    "print(t)\n",
    "\n",
    "t.requires_grad_(False) #trailing underscore modifies in place\n",
    "print(t)\n",
    "t.requires_grad_(True)\n",
    "\n",
    "o = t.detach()\n",
    "print(t)\n",
    "print(o) \n",
    "\n",
    "with torch.no_grad():\n",
    "    y = t + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f640",
   "metadata": {},
   "source": [
    "Whenever we call the backwards function is called, all gradients up to that point in that chain are included and summed. Need to be careful to make sure correct tensors included in calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa28f6",
   "metadata": {},
   "source": [
    "### Simple Example NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85190d7",
   "metadata": {},
   "source": [
    "Gradients important for minimising the loss function by calculating $\\frac{\\partial}{\\partial x} Loss$. The three steps of a neural network are:\n",
    "- Forward pass: Apply functions and calculate loss\n",
    "- Compute partial derivatives at each node on branch diagram wrt each input parameter\n",
    "- Backward pass: Compute derivative of the loss function wrt each input parameter using the chain rule on the derivatives from the previous step\n",
    "\n",
    "Loss is difference between predicted output and the actual output squared. Output is modelled as linear combination of the weights, $\\textbf{w}$ and some input, $\\textbf{x}$, ie $\\hat{y} = \\textbf{w} \\cdot \\textbf{x}$, this makes loss equal to $\\left( \\hat{y} - y \\right)^2 = \\left(\\textbf{w} \\cdot \\textbf{x} - y \\right)^2$. Minimise our loss by calculating the derivative of the loss wrt our weights in the vector $\\textbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ddc0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "#Practice Example\n",
    "x=1\n",
    "y=2\n",
    "w=1\n",
    "#from pen and paper calculation, (wx - y)^2 = 1 and dLoss/dw = 2s(1) = 2(-1)(1) = -2 where s = wx - y\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad=True) #interested in gradient\n",
    "\n",
    "#forward pass and compute loss\n",
    "y_hat = w*x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward() #gradient computation\n",
    "print(w.grad)\n",
    "#works correctly\n",
    "\n",
    "#Next steps: update weights\n",
    "#            do next forward and backward pass\n",
    "#            repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5760eb",
   "metadata": {},
   "source": [
    "### More Concrete Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04611948",
   "metadata": {},
   "source": [
    "Next we make a more concrete model by manually implementing prediction, gradient computation, loss computation and parameter updates with a view to moving to automation using torch's in-built features. This should provide a solid background understanding of how neural networks work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c89ea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 2.200, loss = 44.00000000\n",
      "epoch 2: w = 1.980, loss = 0.44000015\n",
      "epoch 3: w = 2.002, loss = 0.00440001\n",
      "epoch 4: w = 2.000, loss = 0.00004400\n",
      "epoch 5: w = 2.000, loss = 0.00000044\n",
      "epoch 6: w = 2.000, loss = 0.00000000\n",
      "epoch 7: w = 2.000, loss = 0.00000000\n",
      "epoch 8: w = 2.000, loss = 0.00000000\n",
      "epoch 9: w = 2.000, loss = 0.00000000\n",
      "epoch 10: w = 2.000, loss = 0.00000000\n",
      "epoch 11: w = 2.000, loss = 0.00000000\n",
      "epoch 12: w = 2.000, loss = 0.00000000\n",
      "epoch 13: w = 2.000, loss = 0.00000000\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#Linear regression: f = w*x and ignore bias at the moment\n",
    "\n",
    "#Function to approximate: f = 2*x\n",
    "\n",
    "#Training sample\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8, 10], dtype=np.float32)\n",
    "\n",
    "#Initialise weight. Start with 0.0\n",
    "w = 0.0\n",
    "\n",
    "#Manual calculation\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss\n",
    "def loss(y, y_predicted): #y_predicted is the model output ie what is calculated in the forward pass\n",
    "    return ((y_predicted - y)**2).mean() #mean squared error\n",
    "\n",
    "#Gradient\n",
    "#mean squared error = 1/N*(w*x - y)**2\n",
    "#dJ/dw = 1/N*dJ/du*du/dw = 1/N*2u*x = 1/N*2x*(w*x - y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean() #scalar product\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    #gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w -= learning_rate*dw #gradient descent algorithm\n",
    "\n",
    "    if epoch % 1 == 0: #print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda8052",
   "metadata": {},
   "source": [
    "So the key steps are:\n",
    "- Create your input and idealised output arrays (X and Y above).\n",
    "- Initialise the weights\n",
    "- Define forward pass, loss calculation and gradient functions\n",
    "- Pick a learning rate and number of iterations (small learning rate good for fine tuning of weights but can require larger number of iterations)\n",
    "- Create a training loop which repeatedly calls the forward pass, loss and gradient functions \n",
    "- Update the weights in some way so that they eventually minimise the loss function\n",
    "- Once loss is minimised to zero your function is fully approximated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd24da",
   "metadata": {},
   "source": [
    "### Automate Gradient Calculation\n",
    "\n",
    "Above everything is calculated manually. Now to automate using torch, starting with the gradient calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7ddfdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.440, loss = 44.00000000\n",
      "epoch 11: w = 1.870, loss = 0.30573449\n",
      "epoch 21: w = 1.989, loss = 0.00212438\n",
      "epoch 31: w = 1.999, loss = 0.00001476\n",
      "epoch 41: w = 2.000, loss = 0.00000010\n",
      "epoch 51: w = 2.000, loss = 0.00000000\n",
      "epoch 61: w = 2.000, loss = 0.00000000\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#Linear regression: f = w*x and ignore bias at the moment\n",
    "\n",
    "#Function to approximate: f = 2*x\n",
    "\n",
    "#Training sample\n",
    "X = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8, 10], dtype=torch.float32) #now tensors\n",
    "\n",
    "#Initialise weight. Start with 0.0\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) #w also a tensor and we're interested\n",
    "#in the gradient of the loss wrt w we specify that requires_grad=True\n",
    "\n",
    "#Manual calculation\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss\n",
    "def loss(y, y_predicted): #y_predicted is the model output ie what is calculated in the forward pass\n",
    "    return ((y_predicted - y)**2).mean() #mean squared error\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() #dl/dw calculated completely by torch now\n",
    "\n",
    "    #update weights\n",
    "    #Require this to not be included in the gradient branch diagram otherwise the back pass will\n",
    "    #include this step in the calculation of gradient and mess with the network so this becomes...\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad #gradient descent algorithm\n",
    "\n",
    "    #zero gradients. When we call l.backward() it writes gradients and puts them in w.grad() attribute\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0: #print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e4a18",
   "metadata": {},
   "source": [
    "Back propagation not as accurate as the analytical gradient calculation so more iterations required when using torch for back propagation. This seems like an issue for simple networks but comes in handy for more complicated networks where a lot more gradients are needed to be calculated at each node so torch is better than analytical gradient calculation for large networks with many nodes.\n",
    "\n",
    "Adding more data to your inputs and idealised outputs improves convergence time of network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69737c",
   "metadata": {},
   "source": [
    "### Automate Loss and Parameter Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b3ec7",
   "metadata": {},
   "source": [
    "General training pipeline:\n",
    "\n",
    "- Design model (number of inputs and outputs and forward pass with different operations and layers)\n",
    "- Construct loss and optimiser\n",
    "- Training loop:\n",
    "  - Forward pass: Compute prediction\n",
    "  - Backward pass: Compute gradients\n",
    "  - Update weights\n",
    "  - Iterate loop a couple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4ace1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.440, loss = 44.00000000\n",
      "epoch 11: w = 1.870, loss = 0.30573449\n",
      "epoch 21: w = 1.989, loss = 0.00212438\n",
      "epoch 31: w = 1.999, loss = 0.00001476\n",
      "epoch 41: w = 2.000, loss = 0.00000010\n",
      "epoch 51: w = 2.000, loss = 0.00000000\n",
      "epoch 61: w = 2.000, loss = 0.00000000\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "#Copy paste from above\n",
    "#Linear regression: f = w*x and ignore bias at the moment\n",
    "\n",
    "#Function to approximate: f = 2*x\n",
    "\n",
    "#Training sample\n",
    "X = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8, 10], dtype=torch.float32) #now tensors\n",
    "\n",
    "#Initialise weight. Start with 0.0\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) #w also a tensor and we're interested\n",
    "#in the gradient of the loss wrt w we specify that requires_grad=True\n",
    "\n",
    "#Manual calculation\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss automation\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss() #same loss function as before \n",
    "optimiser = torch.optim.SGD([w], lr=learning_rate) #SGD- Stochastic Gradient Descent. torch.optim.SGD requires parameters \n",
    "#to optimise\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    #This part of the training loop can remain the same as above we have loss = nn.MSELoss() which is \n",
    "    #a callable function\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() #dl/dw calculated completely by torch now\n",
    "\n",
    "    #update weights. optimiser does this automatically\n",
    "    optimiser.step()\n",
    "\n",
    "    #zero gradients again for optimiser\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0: #print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed2004",
   "metadata": {},
   "source": [
    "Above the calculation of the loss and optimisation of weights are handled automatically by torch.nn.optim.SGD algorithm. Now to automate the prediction model (forward pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e320d9",
   "metadata": {},
   "source": [
    "### Automate Forward Pass\n",
    "\n",
    "Using torch $\\textit{nn.Linear}$ we can implement a simple linear transformation to the incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5240c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = 3.648\n",
      "epoch 1: w = 0.823, loss = 11.04624844\n",
      "epoch 51: w = 1.764, loss = 0.08088276\n",
      "epoch 101: w = 1.797, loss = 0.05992890\n",
      "epoch 151: w = 1.825, loss = 0.04440355\n",
      "epoch 201: w = 1.849, loss = 0.03290017\n",
      "epoch 251: w = 1.870, loss = 0.02437696\n",
      "epoch 301: w = 1.888, loss = 0.01806181\n",
      "epoch 351: w = 1.904, loss = 0.01338263\n",
      "epoch 401: w = 1.917, loss = 0.00991568\n",
      "epoch 451: w = 1.929, loss = 0.00734691\n",
      "epoch 501: w = 1.939, loss = 0.00544360\n",
      "epoch 551: w = 1.947, loss = 0.00403336\n",
      "epoch 601: w = 1.955, loss = 0.00298846\n",
      "epoch 651: w = 1.961, loss = 0.00221426\n",
      "epoch 701: w = 1.966, loss = 0.00164063\n",
      "epoch 751: w = 1.971, loss = 0.00121560\n",
      "epoch 801: w = 1.975, loss = 0.00090069\n",
      "epoch 851: w = 1.979, loss = 0.00066735\n",
      "epoch 901: w = 1.982, loss = 0.00049447\n",
      "epoch 951: w = 1.984, loss = 0.00036637\n",
      "Prediction after training: f(5) = 9.972\n"
     ]
    }
   ],
   "source": [
    "#Copy paste from above\n",
    "#Linear regression: f = w*x and ignore bias at the moment\n",
    "\n",
    "#Function to approximate: f = 2*x\n",
    "\n",
    "#Training sample\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) #tensors need to be reshaped for nn.Linear\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) #\n",
    "#After reshaping\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features) #returns 4 1 which means there are 4 samples each with one feature\n",
    "#number of rows in input X needs to match the number of samples\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features #Give these to model\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32) #test tensor\n",
    "\n",
    "#Explicit weights and forward pass no longer required as the torch model knows the form we want from its\n",
    "#input parameters:\n",
    "model = nn.Linear(input_size, output_size)\n",
    "#Normally have to design the forward pass ourselves but linear regression is trivial so is provided by \n",
    "#torch off the bat\n",
    "\n",
    "#Loss automation\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}') \n",
    "#model input must be a tensor so create test tensor. Use .item for print statement as model output should\n",
    "#be a scalar\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "loss = nn.MSELoss() #same loss function as before \n",
    "#optimiser also needs modified as we no longer have weights. Change to model.parameters()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate) #SGD- Stochastic Gradient Descent. torch.optim.SGD \n",
    "#requires parameters to optimise\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = model(X) #updated prediction using model instead of forward function from before\n",
    "\n",
    "    #loss\n",
    "    #This part of the training loop can remain the same as above we have loss = nn.MSELoss() which is \n",
    "    #a callable function\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() #dl/dw calculated completely by torch now\n",
    "\n",
    "    #update weights. optimiser does this automatically\n",
    "    optimiser.step()\n",
    "\n",
    "    #zero gradients again for optimiser\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    if epoch % 50 == 0: #print every step\n",
    "        [w, b] = model.parameters() #creates a list of lists of lists\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "#AT FIRST PASS THIS DOESN'T WORK TOO WELL. NEED A LOT MORE ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a6f0490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9718], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight\n",
    "\n",
    "model(torch.tensor([5], dtype=torch.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
