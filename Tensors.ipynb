{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c87921",
   "metadata": {},
   "source": [
    "# Torch Tensors \n",
    "### Notes and code based on youtube videos by user Patrick Loeber: https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73cac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ff3756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([[14, 16],\n",
      "        [19, 17]]) y =  tensor([[1, 4],\n",
      "        [1, 4]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(20, size=(2,2))\n",
    "y = torch.randint(6, size=(2,2))\n",
    "\n",
    "print('x = ', x, 'y = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36f705",
   "metadata": {},
   "source": [
    "Element-wise operations written as $\\textit{x+y, x-y, x*y, x/y}$ or $\\textit{x.add(y)}$. Add a trailing underscore to do the operation in place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7a48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 20],\n",
      "        [20, 21]])\n",
      "tensor([[15, 20],\n",
      "        [20, 21]])\n",
      "Same thing.\n"
     ]
    }
   ],
   "source": [
    "print(x + y)\n",
    "y.add_(x)\n",
    "print(y)\n",
    "if y.all() == (x+y).all():\n",
    "    print(\"Same thing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32066550",
   "metadata": {},
   "source": [
    "Can use .item() to extract (scalar) tensor component as long as the tensor is sliced properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de1d260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1],\n",
      "        [9, 2]])\n",
      "tensor(1) 1\n",
      "<class 'torch.Tensor'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "z = torch.randint(10, (2, 2))\n",
    "print(z)\n",
    "scalar = z[0, 1]\n",
    "print(scalar, scalar.item())\n",
    "print(type(scalar), type(scalar.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157008a",
   "metadata": {},
   "source": [
    "$\\textit{t.view()}$ can be used to reshape a tensor. Reshape dimensions must have the same product as previous dimension product ie $m \\times n$ maps to $a \\times b$ where $mn = ab$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26fb6721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0340, 0.4746, 0.8292, 0.3899],\n",
      "        [0.7197, 0.5315, 0.5914, 0.2985],\n",
      "        [0.3013, 0.6627, 0.2664, 0.9765],\n",
      "        [0.1281, 0.7961, 0.7034, 0.1579]])\n",
      "tensor([[0.0340, 0.4746],\n",
      "        [0.8292, 0.3899],\n",
      "        [0.7197, 0.5315],\n",
      "        [0.5914, 0.2985],\n",
      "        [0.3013, 0.6627],\n",
      "        [0.2664, 0.9765],\n",
      "        [0.1281, 0.7961],\n",
      "        [0.7034, 0.1579]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4, 4)\n",
    "print(a)\n",
    "b = a.view(-1, 2) #reshape -1 makes automatic choice of other dimension when given one dimension as\n",
    "#next input\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f4a6b",
   "metadata": {},
   "source": [
    "Can turn a numpy array into a torch tensor and vice versa. Care needs to be taken though as the tensor and array both occupy the same memory location if only working with the CPU (not GPU) so changes to one are also changes to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840eb305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'numpy.ndarray'>\n",
      "[1. 1. 1. 1. 1.]\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "o = torch.ones(5)\n",
    "print(o)\n",
    "p = o.numpy() \n",
    "print(type(p)) \n",
    "\n",
    "s = np.ones(5)\n",
    "print(s)\n",
    "t = torch.from_numpy(s)\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0df5e",
   "metadata": {},
   "source": [
    "When initialising a tensor, setting the $\\textit{requires\\_grad}$ equal to True let's torch know that you will want to take the gradient of this tensor later on. This attribute needs to be specified before back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65957e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0234, -0.6829,  0.8053], requires_grad=True)\n",
      "tensor([2.0234, 1.3171, 2.8053], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True) \n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1f92c",
   "metadata": {},
   "source": [
    "Since requires_grad is True, torch tracks the operations made in the grad_fn attribute of the tensors and creates a computation graph for back propagation later. The forward pass calculates the output $y = f(x)$ and since gradient is specified, torch automatically creates a function for us which is used in back propagation to calculate the gradient. $y$ has attribute grad_fn which points to gradient function dy/dx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83bca362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.1884,  3.4694, 15.7394], grad_fn=<MulBackward0>)\n",
      "tensor([ 1.6187, 10.5366,  0.0224])\n"
     ]
    }
   ],
   "source": [
    "z = y*y*2\n",
    "print(z)\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype = torch.float32)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb08c7",
   "metadata": {},
   "source": [
    "$\\textit{z.backward()}$ calculates the derivative of z wrt the first tensor in the branch chain ie calculates dz/dx. However, $\\textit{z.backward()}$, doesn't work by itself unless the output is a scalar- it throws the error \"grad can be implicitly created only for scalar outputs\". For it to work you need to perform the vector jacobian product or similar higher dimensional product with the correct rank tensor to output the gradient. The vector jacobian product is similar to a change of basis matrix transformation. This is done by writing $\\textit{z.backward(v)}$ where $v$ is the correct rank tensor required for a properly defined product. Not sure about how the components of v are picked from a neural networks perspective but in the usual jacobian product it usually contains the primary basis coordinates ~.\n",
    "\n",
    "The $\\textit{.grad}$ part outputs a tensor of same rank as the input ($x$ here) containing the \"vector\" in the vector jacobian product, usually the pre-computated gradients wrt each output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729928fc",
   "metadata": {},
   "source": [
    "Actions can be taken to allow a tensor to be created without being added to the branch diagram. This prevents torch from tracking history and calculating the grad_fn attribute allowing for adjustment of the neural network. Can be done in three ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac381d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5792, -0.0888,  0.2685], requires_grad=True)\n",
      "tensor([-0.5792, -0.0888,  0.2685])\n",
      "tensor([-0.5792, -0.0888,  0.2685], requires_grad=True)\n",
      "tensor([-0.5792, -0.0888,  0.2685])\n",
      "tensor([1.4208, 1.9112, 2.2685])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(3, requires_grad=True)\n",
    "print(t)\n",
    "\n",
    "t.requires_grad_(False) #trailing underscore modifies in place\n",
    "print(t)\n",
    "t.requires_grad_(True)\n",
    "\n",
    "o = t.detach()\n",
    "print(t)\n",
    "print(o) \n",
    "\n",
    "with torch.no_grad():\n",
    "    y = t + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f640",
   "metadata": {},
   "source": [
    "Whenever we call the backwards function is called, all gradients up to that point in that chain are included and summed. Need to be careful to make sure correct tensors included in calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85190d7",
   "metadata": {},
   "source": [
    "Gradients important for minimising the loss function by calculating $\\frac{\\partial}{\\partial x} Loss$. The three steps of a neural network are:\n",
    "- Forward pass: Apply functions and calculate loss\n",
    "- Compute partial derivatives at each node on branch diagram wrt each input parameter\n",
    "- Backward pass: Compute derivative of the loss function wrt each input parameter using the chain rule on the derivatives from the previous step\n",
    "\n",
    "Loss is difference between predicted output and the actual output squared. Output is modelled as linear combination of the weights, $\\textbf{w}$ and some input, $\\textbf{x}$, ie $\\hat{y} = \\textbf{w} \\cdot \\textbf{x}$, this makes loss equal to $\\left( \\hat{y} - y \\right)^2 = \\left(\\textbf{w} \\cdot \\textbf{x} - y \\right)^2$. Minimise our loss by calculating the derivative of the loss wrt our weights in the vector $\\textbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddc0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "#Practice Example\n",
    "x=1\n",
    "y=2\n",
    "w=1\n",
    "#from pen and paper calculation, (wx - y)^2 = 1 and dLoss/dw = 2s(1) = 2(-1)(1) = -2 where s = wx - y\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad=True) #interested in gradient\n",
    "\n",
    "#forward pass and compute loss\n",
    "y_hat = w*x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "#backward pass\n",
    "loss.backward() #gradient computation\n",
    "print(w.grad)\n",
    "#works correctly\n",
    "\n",
    "#Next steps: update weights\n",
    "#            do next forward and backward pass\n",
    "#            repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04611948",
   "metadata": {},
   "source": [
    "Next we make a more concrete model by manually implementing prediction, gradient computation, loss computation and parameter updates with a view to moving to automation using torch's in-built features. This should provide a solid background understanding of how neural networks work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c89ea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 2.200, loss = 44.00000000\n",
      "epoch 2: w = 1.980, loss = 0.44000015\n",
      "epoch 3: w = 2.002, loss = 0.00440001\n",
      "epoch 4: w = 2.000, loss = 0.00004400\n",
      "epoch 5: w = 2.000, loss = 0.00000044\n",
      "epoch 6: w = 2.000, loss = 0.00000000\n",
      "epoch 7: w = 2.000, loss = 0.00000000\n",
      "epoch 8: w = 2.000, loss = 0.00000000\n",
      "epoch 9: w = 2.000, loss = 0.00000000\n",
      "epoch 10: w = 2.000, loss = 0.00000000\n",
      "epoch 11: w = 2.000, loss = 0.00000000\n",
      "epoch 12: w = 2.000, loss = 0.00000000\n",
      "epoch 13: w = 2.000, loss = 0.00000000\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#Linear regression: f = w*x and ignore bias at the moment\n",
    "\n",
    "#Function to approximate: f = 2*x\n",
    "\n",
    "#Training sample\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8, 10], dtype=np.float32)\n",
    "\n",
    "#Initialise weight. Start with 0.0\n",
    "w = 0.0\n",
    "\n",
    "#Manual calculation\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss\n",
    "def loss(y, y_predicted): #y_predicted is the model output ie what is calculated in the forward pass\n",
    "    return ((y_predicted - y)**2).mean() #mean squared error\n",
    "\n",
    "#Gradient\n",
    "#mean squared error = 1/N*(w*x - y)**2\n",
    "#dJ/dw = 1/N*dJ/du*du/dw = 1/N*2u*x = 1/N*2x*(w*x - y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean() #scalar product\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    #gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w -= learning_rate*dw #gradient descent algorithm\n",
    "\n",
    "    if epoch % 1 == 0: #print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda8052",
   "metadata": {},
   "source": [
    "So the key steps are:\n",
    "- Create your input and idealised output arrays (X and Y above).\n",
    "- Initialise the weights\n",
    "- Define forward pass, loss calculation and gradient functions\n",
    "- Pick a learning rate and number of iterations (small learning rate good for fine tuning of weights but can require larger number of iterations)\n",
    "- Create a training loop which repeatedly calls the forward pass, loss and gradient functions \n",
    "- Update the weights in some way so that they eventually minimise the loss function\n",
    "- Once loss is minimised to zero your function is fully approximated\n",
    "\n",
    "Above everything is calculated manually. Now to automate using torch, starting with the gradient calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddfdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.440, loss = 44.00000000\n",
      "epoch 5: w = 1.423, loss = 6.02850342\n",
      "epoch 9: w = 1.786, loss = 0.82597363\n",
      "epoch 13: w = 1.921, loss = 0.11316784\n",
      "epoch 17: w = 1.971, loss = 0.01550541\n",
      "epoch 21: w = 1.989, loss = 0.00212438\n",
      "epoch 25: w = 1.996, loss = 0.00029106\n",
      "epoch 29: w = 1.999, loss = 0.00003988\n",
      "epoch 33: w = 1.999, loss = 0.00000546\n",
      "epoch 37: w = 2.000, loss = 0.00000075\n",
      "epoch 41: w = 2.000, loss = 0.00000010\n",
      "epoch 45: w = 2.000, loss = 0.00000001\n",
      "epoch 49: w = 2.000, loss = 0.00000000\n",
      "epoch 53: w = 2.000, loss = 0.00000000\n",
      "epoch 57: w = 2.000, loss = 0.00000000\n",
      "epoch 61: w = 2.000, loss = 0.00000000\n",
      "epoch 65: w = 2.000, loss = 0.00000000\n",
      "epoch 69: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "#Linear regression: f = w*x and ignore bias at the moment\n",
    "\n",
    "#Function to approximate: f = 2*x\n",
    "\n",
    "#Training sample\n",
    "X = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8, 10], dtype=torch.float32) #now tensors\n",
    "\n",
    "#Initialise weight. Start with 0.0\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) #w also a tensor and we're interested\n",
    "#in the gradient of the loss wrt w we specify that requires_grad=True\n",
    "\n",
    "#Manual calculation\n",
    "#Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#Loss\n",
    "def loss(y, y_predicted): #y_predicted is the model output ie what is calculated in the forward pass\n",
    "    return ((y_predicted - y)**2).mean() #mean squared error\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "#Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() #dl/dw calculated completely by torch now\n",
    "\n",
    "    #update weights\n",
    "    #Require this to not be included in the gradient branch diagram otherwise the back pass will\n",
    "    #include this step in the calculation of gradient and mess with the network so this becomes...\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad #gradient descent algorithm\n",
    "\n",
    "    #zero gradients. When we call l.backward() it writes gradients and puts them in w.grad() attribute\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0: #print every step\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e4a18",
   "metadata": {},
   "source": [
    "Back propagation not as accurate as the analytical gradient calculation so more iterations required when using torch for back propagation. This seems like an issue for simple networks but comes in handy for more complicated networks where a lot more gradients are needed to be calculated at each node so torch is better than analytical gradient calculation for large networks with many nodes.\n",
    "\n",
    "Adding more data to your inputs and idealised outputs improves convergence time of network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
