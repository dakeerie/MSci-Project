{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242e5660",
   "metadata": {},
   "source": [
    "# Damped Harmonic Oscillator \n",
    "\n",
    "Damped harmonic oscillator takes form:\n",
    "\n",
    "$y'' + 2\\gamma y' + \\omega_0^2 y = 0$ and has general solution:\n",
    "\n",
    "$y(t) = A \\exp{\\left(\\left( -\\gamma + \\sqrt{\\gamma^2 -\\omega_0^2}\\right)t\\right)} + B \\exp{\\left(\\left( -\\gamma - \\sqrt{\\gamma^2 -\\omega_0^2}\\right)t\\right)}$\n",
    "\n",
    "Require $\\omega_0^2 > \\gamma^2$ for underdamped oscilation. For this scenario, the general solution becomes\n",
    "\n",
    "$y(t) = C e^{- \\gamma t} \\left(\\exp{i (\\sqrt{\\omega_0^2 - \\gamma^2}) t} + \\exp{i (\\sqrt{\\omega_0^2 - \\gamma^2}) t}\\right)\\\\$\n",
    "$\\Rightarrow y(t) = 2C e^{-\\gamma t} \\cos{\\left(\\omega t + \\phi\\right)} \\\\$\n",
    "\n",
    "where $\\omega = \\sqrt{\\omega^2_0 -\\gamma^2}$ and $\\phi$ is some phase offset determined by the initial conditions. We will take $\\phi = 0$ so the oscillator is at maximum displacement at $t=0$. \n",
    "\n",
    "Aim to make a neural network to approximate the harmonic oscillator function, initialised with random weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98843985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F #contains various functions including activation functions \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f69d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analytical Solution\n",
    "#A = 2C here\n",
    "\n",
    "def analytical(time, gamma, omega_0, A):\n",
    "    omega = np.sqrt(omega_0**2 - gamma**2)\n",
    "    envelope = A*np.exp(-gamma*time)\n",
    "    return envelope*np.cos(omega*time), envelope\n",
    "\n",
    "#System parameters\n",
    "gamma = 2\n",
    "omega_0 = 20\n",
    "A = 1\n",
    "\n",
    "t_analytical = np.linspace(0, 1, 500)\n",
    "y_analytical = analytical(t_analytical, gamma, omega_0, A)\n",
    "t_tensor = t.tensor(t_analytical, dtype= t.float32, requires_grad=True).view(-1,1)\n",
    "t_initial = t.tensor(0., requires_grad=True).view(-1, 1) #t=0 tensor\n",
    "\n",
    "\n",
    "\n",
    "# with t.no_grad():\n",
    "#     print(t_initial, t_initial.shape)\n",
    "#     print(t_analytical, t_analytical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab0e260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): #to call nn.Module to inherit pytorch nn functionality\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, num_hidden_layers=2):\n",
    "        #This function initialises the neural network, setting up the infrastructure to be used by forward\n",
    "        #hidden_channels parameter is the number of neurons in each hidden layer\n",
    "        super().__init__() #initialise nn.Module parent class first to use nn infrastructure\n",
    "        self.input_layer = nn.Linear(in_channels, hidden_channels) #transforms input to match hidden\n",
    "    \n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_channels, hidden_channels) for _ in range(num_hidden_layers)]) \n",
    "        #creates {num_hidden_layers} hidden layers and adds a linear transformation \n",
    "        #with its own trainable weights and bias between each layer and ensures the inputs\n",
    "        #and outputs of each hidden layer match the next.\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_channels, out_channels) #transforms output of hidden layers\n",
    "        #to the shape of the output channels\n",
    "\n",
    "    def forward(self, x: t.tensor):\n",
    "        #defines how data flows through the network \n",
    "        #x is a tensor with shape [batch_size, in_channels]\n",
    "        x = F.tanh(self.input_layer(x)) #tanh activation function after first layer, introduces non-linearity\n",
    "        #x leaves as tensor with shape [batch_size, hidden_channels]\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.sigmoid(hidden_layer(x)) #iterates through all hidden layers, applies linear transformation\n",
    "            #and tanh activation\n",
    "        #x leaves as tensor with shape [batch_size, output_channels]\n",
    "        x = self.output_layer(x) #no activation function here\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a13b06",
   "metadata": {},
   "source": [
    "Above code defines the neural network structure and the forward pass ie the neural network model. Key features are the requirement of inheriting the nn.Module class to make use of the pytorch neural network features off the bat- allows parameter tracking and easy gradient calculation. The number of hidden layers and neurons in each hidden layer can be tuned. Each layer applies a linear transformation to reshape its input into the correct shape to be passed on to the next layer, each transformation has its own weights and bias which can be trained. Use of super.__init__() allows layers to be looped through, indexed and their parameters can be accessed using list(model.parameters()). forward function adds tanh  activation functions after each linear transformation before being passed on to next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9186d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3000. Total scaled loss: 3.0596e+00, \n",
      "                    y(0) loss: 1.2300e+00, \n",
      "                    y'(0) loss: 2.0150e-05, \n",
      "                    physics loss: 1.8297e+03\n",
      "Epoch: 500 / 3000. Total scaled loss: 6.9271e-01, \n",
      "                    y(0) loss: 5.5613e-01, \n",
      "                    y'(0) loss: 3.5518e-02, \n",
      "                    physics loss: 1.3303e+02\n",
      "Epoch: 1000 / 3000. Total scaled loss: 3.1285e-01, \n",
      "                    y(0) loss: 1.0800e-01, \n",
      "                    y'(0) loss: 4.6103e-05, \n",
      "                    physics loss: 2.0485e+02\n",
      "Epoch: 1500 / 3000. Total scaled loss: 1.5861e-01, \n",
      "                    y(0) loss: 3.8604e-02, \n",
      "                    y'(0) loss: 8.3767e-07, \n",
      "                    physics loss: 1.2001e+02\n",
      "Epoch: 2000 / 3000. Total scaled loss: 8.0832e-02, \n",
      "                    y(0) loss: 1.1302e-02, \n",
      "                    y'(0) loss: 8.8938e-03, \n",
      "                    physics loss: 6.8640e+01\n",
      "Epoch: 2500 / 3000. Total scaled loss: 4.8955e-02, \n",
      "                    y(0) loss: 3.9675e-03, \n",
      "                    y'(0) loss: 9.7033e-06, \n",
      "                    physics loss: 4.4986e+01\n",
      "Epoch: 3000 / 3000. Total scaled loss: 7.7047e-03, \n",
      "                    y(0) loss: 5.7182e-04, \n",
      "                    y'(0) loss: 6.3442e-05, \n",
      "                    physics loss: 7.1265e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Next step is loss and optimiser\n",
    "learning_rate = 1e-2\n",
    "model = Model(1, 1, 32)\n",
    "# with t.no_grad():\n",
    "#     for param in model.parameters():\n",
    "#         print(param.shape, param[0]) #randomised weights and biases\n",
    "optimiser = optim.Adam(model.parameters(), lr = learning_rate) #pass optimiser the trainable parameters\n",
    "\n",
    "number_iterations = 3000\n",
    "loss_final = []\n",
    "avg_residuals = []\n",
    "for epoch in range(number_iterations):\n",
    "    y_predictedIC = model(t_initial)\n",
    "    loss_y_IC = (t.squeeze(y_predictedIC) - A)**2 #start at maximum amplitude, squeeze returns input without \n",
    "    #specified dimensions of 1\n",
    "\n",
    "    dy_predictedIC = t.autograd.grad(y_predictedIC, t_initial, t.ones_like(y_predictedIC), create_graph=True)[0]\n",
    "    #above calculates the gradient of y_predicted wrt t_initial. t.ones_like gives vector of correct shape \n",
    "    #for vector jacobian product. ones_like gives direction of derivative and sums each derivative (1*y_1 + ... + 1*y_n). \n",
    "    #create_graph=True allows higher order derivatives to be created from this gradient object. \n",
    "    loss_dy_IC = (t.squeeze(dy_predictedIC) - 0)**2 #start with zero velocity\n",
    "\n",
    "    y_predicted = model(t_tensor)\n",
    "    dy_predicted = t.autograd.grad(y_predicted, t_tensor, t.ones_like(y_predicted), create_graph=True)[0]\n",
    "    d2y_predicted = t.autograd.grad(dy_predicted, t_tensor, t.ones_like(dy_predicted), create_graph=True)[0]\n",
    "    loss_y_sol = t.mean((d2y_predicted + 2*gamma*dy_predicted + omega_0**2*y_predicted)**2)\n",
    "\n",
    "    loss = loss_y_sol*1e-3 + loss_y_IC + loss_dy_IC*1e-1 #start with equal scalings but if one is much smaller than\n",
    "    #others then will effectively be ignored. Think about relative values of each loss and adjust accordingly.\n",
    "    #Print losses at different steps to check.\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    optimiser.zero_grad()\n",
    "    with t.no_grad():\n",
    "        loss_final.append(loss.detach().numpy())\n",
    "\n",
    "    y_nn = model(t_tensor)\n",
    "    res = np.mean(np.abs(y_nn.detach().numpy() - y_analytical[0]))\n",
    "    avg_residuals.append(res)\n",
    "\n",
    "    if (epoch + 1) % 500 == 0 or epoch == 0:\n",
    "        print(f\"\"\"Epoch: {epoch + 1} / {number_iterations}. Total scaled loss: {loss:.4e}, \n",
    "                    y(0) loss: {loss_y_IC:.4e}, \n",
    "                    y'(0) loss: {loss_dy_IC:.4e}, \n",
    "                    physics loss: {loss_y_sol:.4e}\"\"\")\n",
    "        plt.figure(figsize = [4, 4])\n",
    "        plt.plot(t_tensor.detach().numpy(), y_nn.detach().numpy(), label = ' Neural Network')\n",
    "        plt.plot(t_analytical, y_analytical[0], label = 'Analytical')\n",
    "        plt.plot(t_analytical, y_analytical[1], linestyle = '--', color = 'red', label = 'Decay Envelope')\n",
    "        plt.plot(t_analytical, -y_analytical[1], linestyle = '--', color = 'red')\n",
    "        plt.title(f'Epoch: {epoch + 1}'\n",
    "                  '\\n'\n",
    "                  f'Total Loss: {loss:.2e}', fontsize = 15)\n",
    "        plt.xlabel('Time (s)', fontsize = 12)\n",
    "        plt.ylabel('Displacement (m)', fontsize = 12)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'DHOFigures/DHOEpoch{epoch + 1}.png', format = 'png')\n",
    "        plt.clf()\n",
    "\n",
    "epoch_axis = np.linspace(0, number_iterations, number_iterations)\n",
    "plt.figure(figsize = [4,4])\n",
    "plt.plot(epoch_axis, loss_final, color = 'green')\n",
    "plt.xlabel('Epoch', fontsize = 15)\n",
    "plt.ylabel('Total loss', fontsize = 15)\n",
    "plt.title('Total loss vs. Epoch', fontsize = 20)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('DHOFigures/Loss vs t.png', format = 'png')\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize = [4,4])\n",
    "plt.plot(epoch_axis, avg_residuals, color = 'green')\n",
    "plt.xlabel('Epoch', fontsize = 15)\n",
    "plt.ylabel('Average Residual', fontsize = 15)\n",
    "plt.title('Average Residual vs. Epoch', fontsize = 20)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('DHOFigures/Average Residual.png', format = 'png')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba64b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
